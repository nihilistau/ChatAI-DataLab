{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcc834e4",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "# Hypothesis Workflow Control Lab\n",
    "\n",
    "All-in-one environment dedicated to designing, validating, and steering hypotheses inside ChatAI · DataLab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921613ad",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "### Title Panel · Mission & Scope\n",
    "\n",
    "**Flow:** Manage every step from hypothesis ideation → experiment design → execution telemetry → decision logs without leaving this notebook.\n",
    "\n",
    "**Usage**\n",
    "\n",
    "- Pin hypotheses you care about, tag them, and set expectations for the tests that must validate them.\n",
    "- Use the experiment designer to add tests, compose combined experiments, and simulate or log real runs.\n",
    "- Monitor Tail/Ops logs directly from the notebook so the control surface stays in sync with the front-end Ops Deck.\n",
    "\n",
    "**Hints**\n",
    "\n",
    "- Press the refresh buttons on each panel after running real automation so that live metrics (votes, pass rates, ops logs) stay aligned.\n",
    "- The voting + decision matrix panel helps you choose which hypothesis to materialize in the TestLab environment.\n",
    "- Every action emits a change-log entry so you always know “how many changes” have happened in this working session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ebf49",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "### Layout & Modules\n",
    "\n",
    "1. **Meta grid** — top cards report hypothesis counts, pass rates, votes, change-log volume, and captured data points.\n",
    "2. **Flow explainer** — a narrative panel that details what inputs you have, what actions are available, and how to interpret outputs.\n",
    "3. **Experiment designer** — create, combine, and run tests (manually or via simulation) while keeping an audit of their states.\n",
    "4. **Decision + voting matrix** — compare confidence, votes, and data volume before deciding which hypothesis to promote.\n",
    "5. **Interactive data wall** — sliders and toggles to visualize relationships (e.g., votes vs. pass rate, latency trends, data volume per stage).\n",
    "6. **Ops/Tail console** — pulls the same `/api/tail-log` feed used by the Ops Deck so both surfaces share the latest actions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "475609b8",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ":root {\n",
       "    --lab-bg: #05060d;\n",
       "    --lab-panel: #0b1020;\n",
       "    --lab-panel-alt: #10172b;\n",
       "    --lab-border: #1c2340;\n",
       "    --lab-accent: #9d7bff;\n",
       "    --lab-lime: #b5f36a;\n",
       "    --lab-amber: #ffb347;\n",
       "    --lab-salmon: #ff9ca8;\n",
       "    --lab-text: #f0f4ff;\n",
       "}\n",
       "\n",
       ".lab-panel {\n",
       "    background: var(--lab-panel);\n",
       "    border: 1px solid var(--lab-border);\n",
       "    border-radius: 18px;\n",
       "    padding: 1.15rem 1.35rem;\n",
       "    color: var(--lab-text);\n",
       "    box-shadow: inset 0 0 35px rgba(0, 0, 0, 0.35);\n",
       "}\n",
       "\n",
       ".lab-panel h3, .lab-panel h4 {\n",
       "    margin-top: 0;\n",
       "}\n",
       "\n",
       ".meta-grid {\n",
       "    display: grid;\n",
       "    grid-template-columns: repeat(auto-fit, minmax(160px, 1fr));\n",
       "    gap: 0.9rem;\n",
       "}\n",
       "\n",
       ".stat-card {\n",
       "    border: 1px solid rgba(255, 255, 255, 0.12);\n",
       "    border-radius: 14px;\n",
       "    padding: 0.9rem 1rem;\n",
       "    background: rgba(8, 12, 24, 0.75);\n",
       "}\n",
       "\n",
       ".stat-card h4 {\n",
       "    margin: 0;\n",
       "    font-size: 0.85rem;\n",
       "    text-transform: uppercase;\n",
       "    letter-spacing: 0.2em;\n",
       "    color: var(--lab-amber);\n",
       "}\n",
       "\n",
       ".stat-card p {\n",
       "    margin: 0.35rem 0 0;\n",
       "    font-size: 1.65rem;\n",
       "    font-weight: 600;\n",
       "}\n",
       "\n",
       ".lab-table {\n",
       "    border-collapse: collapse;\n",
       "    width: 100%;\n",
       "}\n",
       "\n",
       ".lab-table th, .lab-table td {\n",
       "    border: 1px solid rgba(255, 255, 255, 0.08);\n",
       "    padding: 0.45rem 0.65rem;\n",
       "    font-size: 0.9rem;\n",
       "}\n",
       "\n",
       ".lab-table th {\n",
       "    text-transform: uppercase;\n",
       "    letter-spacing: 0.15em;\n",
       "    font-size: 0.75rem;\n",
       "    color: var(--lab-amber);\n",
       "}\n",
       "\n",
       ".ops-log-entry {\n",
       "    border-bottom: 1px solid rgba(255, 255, 255, 0.08);\n",
       "    padding: 0.35rem 0;\n",
       "    font-family: \"JetBrains Mono\", \"Fira Code\", monospace;\n",
       "}\n",
       "\n",
       ".vote-pill {\n",
       "    display: inline-flex;\n",
       "    align-items: center;\n",
       "    gap: 0.4rem;\n",
       "    border: 1px solid rgba(255, 255, 255, 0.18);\n",
       "    border-radius: 999px;\n",
       "    padding: 0.15rem 0.75rem;\n",
       "    font-size: 0.8rem;\n",
       "    text-transform: uppercase;\n",
       "    letter-spacing: 0.2em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import statistics\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "from uuid import uuid4\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import HTML, clear_output, display\n",
    "\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except ImportError:\n",
    "    requests = None\n",
    "\n",
    "THEME_STYLES = \"\"\"\n",
    "<style>\n",
    ":root {\n",
    "    --lab-bg: #05060d;\n",
    "    --lab-panel: #0b1020;\n",
    "    --lab-panel-alt: #10172b;\n",
    "    --lab-border: #1c2340;\n",
    "    --lab-accent: #9d7bff;\n",
    "    --lab-lime: #b5f36a;\n",
    "    --lab-amber: #ffb347;\n",
    "    --lab-salmon: #ff9ca8;\n",
    "    --lab-text: #f0f4ff;\n",
    "}\n",
    "\n",
    ".lab-panel {\n",
    "    background: var(--lab-panel);\n",
    "    border: 1px solid var(--lab-border);\n",
    "    border-radius: 18px;\n",
    "    padding: 1.15rem 1.35rem;\n",
    "    color: var(--lab-text);\n",
    "    box-shadow: inset 0 0 35px rgba(0, 0, 0, 0.35);\n",
    "}\n",
    "\n",
    ".lab-panel h3, .lab-panel h4 {\n",
    "    margin-top: 0;\n",
    "}\n",
    "\n",
    ".meta-grid {\n",
    "    display: grid;\n",
    "    grid-template-columns: repeat(auto-fit, minmax(160px, 1fr));\n",
    "    gap: 0.9rem;\n",
    "}\n",
    "\n",
    ".stat-card {\n",
    "    border: 1px solid rgba(255, 255, 255, 0.12);\n",
    "    border-radius: 14px;\n",
    "    padding: 0.9rem 1rem;\n",
    "    background: rgba(8, 12, 24, 0.75);\n",
    "}\n",
    "\n",
    ".stat-card h4 {\n",
    "    margin: 0;\n",
    "    font-size: 0.85rem;\n",
    "    text-transform: uppercase;\n",
    "    letter-spacing: 0.2em;\n",
    "    color: var(--lab-amber);\n",
    "}\n",
    "\n",
    ".stat-card p {\n",
    "    margin: 0.35rem 0 0;\n",
    "    font-size: 1.65rem;\n",
    "    font-weight: 600;\n",
    "}\n",
    "\n",
    ".lab-table {\n",
    "    border-collapse: collapse;\n",
    "    width: 100%;\n",
    "}\n",
    "\n",
    ".lab-table th, .lab-table td {\n",
    "    border: 1px solid rgba(255, 255, 255, 0.08);\n",
    "    padding: 0.45rem 0.65rem;\n",
    "    font-size: 0.9rem;\n",
    "}\n",
    "\n",
    ".lab-table th {\n",
    "    text-transform: uppercase;\n",
    "    letter-spacing: 0.15em;\n",
    "    font-size: 0.75rem;\n",
    "    color: var(--lab-amber);\n",
    "}\n",
    "\n",
    ".ops-log-entry {\n",
    "    border-bottom: 1px solid rgba(255, 255, 255, 0.08);\n",
    "    padding: 0.35rem 0;\n",
    "    font-family: \"JetBrains Mono\", \"Fira Code\", monospace;\n",
    "}\n",
    "\n",
    ".vote-pill {\n",
    "    display: inline-flex;\n",
    "    align-items: center;\n",
    "    gap: 0.4rem;\n",
    "    border: 1px solid rgba(255, 255, 255, 0.18);\n",
    "    border-radius: 999px;\n",
    "    padding: 0.15rem 0.75rem;\n",
    "    font-size: 0.8rem;\n",
    "    text-transform: uppercase;\n",
    "    letter-spacing: 0.2em;\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(THEME_STYLES))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5df0ebd8",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def slugify(value: str, prefix: str = \"hyp\") -> str:\n",
    "    base = \"\".join(ch.lower() if ch.isalnum() else \"-\" for ch in value).strip(\"-\")\n",
    "    base = \"-\".join(part for part in base.split(\"-\") if part)\n",
    "    token = base or f\"{prefix}-{uuid4().hex[:4]}\"\n",
    "    return token\n",
    "\n",
    "@dataclass\n",
    "class Hypothesis:\n",
    "    key: str\n",
    "    title: str\n",
    "    objective: str\n",
    "    owner: str = \"shared\"\n",
    "    tags: List[str] = field(default_factory=list)\n",
    "    confidence: float = 0.5\n",
    "    votes: int = 0\n",
    "    stage: str = \"ideation\"\n",
    "    data_points: int = 0\n",
    "\n",
    "@dataclass\n",
    "class TestCase:\n",
    "    key: str\n",
    "    hypothesis_key: str\n",
    "    name: str\n",
    "    description: str\n",
    "    metric: str\n",
    "    target: float\n",
    "    stage: str = \"lab\"\n",
    "    owner: str = \"shared\"\n",
    "    weight: float = 1.0\n",
    "    last_run: Optional[datetime] = None\n",
    "    status: str = \"pending\"\n",
    "\n",
    "@dataclass\n",
    "class TestRun:\n",
    "    run_id: str\n",
    "    test_key: str\n",
    "    hypothesis_key: str\n",
    "    value: float\n",
    "    status: str\n",
    "    notes: str\n",
    "    sample_size: int\n",
    "    created_at: datetime\n",
    "\n",
    "class HypothesisRegistry:\n",
    "    def __init__(\n",
    "        self,\n",
    "        hypotheses: Optional[List[Hypothesis]] = None,\n",
    "        tests: Optional[List[TestCase]] = None,\n",
    "        runs: Optional[List[TestRun]] = None,\n",
    "    ) -> None:\n",
    "        self.hypotheses: Dict[str, Hypothesis] = {}\n",
    "        self.tests: Dict[str, TestCase] = {}\n",
    "        self.runs: List[TestRun] = []\n",
    "        self.changelog: List[tuple[datetime, str]] = []\n",
    "        if hypotheses:\n",
    "            for hyp in hypotheses:\n",
    "                self.hypotheses[hyp.key] = hyp\n",
    "                self.record_change(f\"seeded hypothesis · {hyp.title}\")\n",
    "        if tests:\n",
    "            for test in tests:\n",
    "                self.tests[test.key] = test\n",
    "                self.record_change(f\"seeded test · {test.name}\")\n",
    "        if runs:\n",
    "            for run in runs:\n",
    "                self.runs.append(run)\n",
    "        self.recalculate_confidence()\n",
    "\n",
    "    def record_change(self, message: str) -> None:\n",
    "        self.changelog.append((datetime.utcnow(), message))\n",
    "        if len(self.changelog) > 200:\n",
    "            self.changelog = self.changelog[-200:]\n",
    "\n",
    "    def recalculate_confidence(self) -> None:\n",
    "        grouped: Dict[str, List[bool]] = {}\n",
    "        for run in self.runs:\n",
    "            grouped.setdefault(run.hypothesis_key, []).append(run.status == \"pass\")\n",
    "        for key, hyp in self.hypotheses.items():\n",
    "            verdicts = grouped.get(key, [])\n",
    "            hyp.confidence = round(0.35 + 0.55 * (sum(verdicts) / len(verdicts)), 3) if verdicts else hyp.confidence\n",
    "            hyp.data_points = sum(r.sample_size for r in self.runs if r.hypothesis_key == key)\n",
    "\n",
    "    def add_hypothesis(self, title: str, objective: str, owner: str = \"shared\", tags: Optional[List[str]] = None) -> Hypothesis:\n",
    "        key = slugify(title)\n",
    "        if key in self.hypotheses:\n",
    "            key = f\"{key}-{len(self.hypotheses)}\"\n",
    "        hyp = Hypothesis(key=key, title=title, objective=objective, owner=owner, tags=tags or [])\n",
    "        self.hypotheses[key] = hyp\n",
    "        self.record_change(f\"added hypothesis · {title}\")\n",
    "        return hyp\n",
    "\n",
    "    def add_test(\n",
    "        self,\n",
    "        hypothesis_key: str,\n",
    "        name: str,\n",
    "        description: str,\n",
    "        metric: str,\n",
    "        target: float,\n",
    "        stage: str = \"lab\",\n",
    "        owner: str = \"shared\",\n",
    "        weight: float = 1.0,\n",
    "    ) -> TestCase:\n",
    "        key = slugify(name, prefix=\"test\")\n",
    "        if key in self.tests:\n",
    "            key = f\"{key}-{len(self.tests)}\"\n",
    "        test = TestCase(\n",
    "            key=key,\n",
    "            hypothesis_key=hypothesis_key,\n",
    "            name=name,\n",
    "            description=description,\n",
    "            metric=metric,\n",
    "            target=target,\n",
    "            stage=stage,\n",
    "            owner=owner,\n",
    "            weight=weight,\n",
    "        )\n",
    "        self.tests[key] = test\n",
    "        self.record_change(f\"added test · {name}\")\n",
    "        return test\n",
    "\n",
    "    def log_run(self, test_key: str, value: float, notes: str = \"\", sample_size: int = 120) -> TestRun:\n",
    "        test = self.tests[test_key]\n",
    "        status = \"pass\" if value >= test.target else \"fail\"\n",
    "        run = TestRun(\n",
    "            run_id=f\"run-{uuid4().hex[:6]}\",\n",
    "            test_key=test.key,\n",
    "            hypothesis_key=test.hypothesis_key,\n",
    "            value=float(value),\n",
    "            status=status,\n",
    "            notes=notes or \"manual entry\",\n",
    "            sample_size=int(sample_size),\n",
    "            created_at=datetime.utcnow(),\n",
    "        )\n",
    "        self.runs.append(run)\n",
    "        test.last_run = run.created_at\n",
    "        test.status = status\n",
    "        self.record_change(f\"{test.name} {status} @ {run.value:.2f}\")\n",
    "        self.recalculate_confidence()\n",
    "        return run\n",
    "\n",
    "    def simulate_run(self, test_key: str, intensity: float = 1.0, jitter: float = 0.08) -> TestRun:\n",
    "        test = self.tests[test_key]\n",
    "        centered = test.target * random.uniform(0.92, 1.08) * intensity\n",
    "        noise = random.gauss(0, test.target * jitter)\n",
    "        value = max(centered + noise, 0)\n",
    "        notes = f\"simulated · intensity={intensity:.2f}\"\n",
    "        sample = random.randint(60, 240)\n",
    "        return self.log_run(test_key=test_key, value=value, notes=notes, sample_size=sample)\n",
    "\n",
    "    def tests_frame(self) -> pd.DataFrame:\n",
    "        rows = []\n",
    "        for test in self.tests.values():\n",
    "            runs = [run for run in self.runs if run.test_key == test.key]\n",
    "            pass_rate = sum(run.status == \"pass\" for run in runs) / len(runs) if runs else 0\n",
    "            avg_value = statistics.mean(run.value for run in runs) if runs else None\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"hypothesis_key\": test.hypothesis_key,\n",
    "                    \"hypothesis\": self.hypotheses[test.hypothesis_key].title,\n",
    "                    \"test_key\": test.key,\n",
    "                    \"test\": test.name,\n",
    "                    \"metric\": test.metric,\n",
    "                    \"target\": test.target,\n",
    "                    \"stage\": test.stage,\n",
    "                    \"owner\": test.owner,\n",
    "                    \"last_run\": test.last_run.isoformat() if test.last_run else None,\n",
    "                    \"status\": test.status,\n",
    "                    \"pass_rate\": round(pass_rate, 3),\n",
    "                    \"avg_value\": round(avg_value, 3) if avg_value is not None else None,\n",
    "                    \"data_points\": sum(run.sample_size for run in runs),\n",
    "                    \"run_count\": len(runs),\n",
    "                }\n",
    "            )\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def hypotheses_frame(self) -> pd.DataFrame:\n",
    "        df = self.tests_frame()\n",
    "        rows = []\n",
    "        for hyp in self.hypotheses.values():\n",
    "            subset = df[df[\"hypothesis_key\"] == hyp.key]\n",
    "            pass_rate = subset[\"pass_rate\"].mean() if not subset.empty else 0\n",
    "            run_count = subset[\"run_count\"].sum() if not subset.empty else 0\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"key\": hyp.key,\n",
    "                    \"title\": hyp.title,\n",
    "                    \"stage\": hyp.stage,\n",
    "                    \"objective\": hyp.objective,\n",
    "                    \"tags\": \", \".join(hyp.tags),\n",
    "                    \"confidence\": hyp.confidence,\n",
    "                    \"votes\": hyp.votes,\n",
    "                    \"data_points\": hyp.data_points,\n",
    "                    \"avg_pass_rate\": round(pass_rate, 3),\n",
    "                    \"run_count\": run_count,\n",
    "                }\n",
    "            )\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def results_frame(self) -> pd.DataFrame:\n",
    "        rows = [\n",
    "            {\n",
    "                \"run_id\": run.run_id,\n",
    "                \"test_key\": run.test_key,\n",
    "                \"hypothesis_key\": run.hypothesis_key,\n",
    "                \"value\": run.value,\n",
    "                \"status\": run.status,\n",
    "                \"notes\": run.notes,\n",
    "                \"sample_size\": run.sample_size,\n",
    "                \"created_at\": run.created_at,\n",
    "                \"delta_vs_target\": run.value - self.tests[run.test_key].target,\n",
    "            }\n",
    "            for run in self.runs\n",
    "        ]\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def metrics(self) -> Dict[str, float]:\n",
    "        tests_df = self.tests_frame()\n",
    "        hyp_df = self.hypotheses_frame()\n",
    "        return {\n",
    "            \"hypotheses\": len(self.hypotheses),\n",
    "            \"tests\": len(self.tests),\n",
    "            \"active_tests\": int((tests_df[\"status\"] == \"pass\").sum()) if not tests_df.empty else 0,\n",
    "            \"avg_confidence\": float(hyp_df[\"confidence\"].mean()) if not hyp_df.empty else 0,\n",
    "            \"votes\": int(hyp_df[\"votes\"].sum()) if not hyp_df.empty else 0,\n",
    "            \"data_points\": int(hyp_df[\"data_points\"].sum()) if not hyp_df.empty else 0,\n",
    "            \"changes\": len(self.changelog),\n",
    "            \"runs\": len(self.runs),\n",
    "        }\n",
    "\n",
    "    def combine_tests(self, test_keys: List[str]) -> pd.DataFrame:\n",
    "        df = self.tests_frame()\n",
    "        subset = df[df[\"test_key\"].isin(test_keys)]\n",
    "        if subset.empty:\n",
    "            return pd.DataFrame()\n",
    "        combo = (\n",
    "            subset.groupby([\"hypothesis\", \"stage\"])\n",
    "            .agg(\n",
    "                pass_rate=(\"pass_rate\", \"mean\"),\n",
    "                avg_target=(\"target\", \"mean\"),\n",
    "                avg_value=(\"avg_value\", \"mean\"),\n",
    "                total_runs=(\"run_count\", \"sum\"),\n",
    "                total_points=(\"data_points\", \"sum\"),\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        combo[\"delta_vs_target\"] = combo[\"avg_value\"] - combo[\"avg_target\"]\n",
    "        return combo\n",
    "\n",
    "    def cast_vote(self, hypothesis_key: str, votes: int) -> None:\n",
    "        hyp = self.hypotheses[hypothesis_key]\n",
    "        hyp.votes += votes\n",
    "        self.record_change(f\"votes +{votes} → {hyp.title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b59ddc",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nihil\\AppData\\Local\\Temp\\ipykernel_32780\\1838767404.py:69: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  self.changelog.append((datetime.utcnow(), message))\n",
      "C:\\Users\\Nihil\\AppData\\Local\\Temp\\ipykernel_32780\\1838767404.py:131: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  created_at=datetime.utcnow(),\n"
     ]
    }
   ],
   "source": [
    "# Seed data using the existing ChatAI canvas hypotheses.\n",
    "sample_hypotheses = [\n",
    "    Hypothesis(\n",
    "        key=\"hyp-pause-density\",\n",
    "        title=\"Pause density telemetry\",\n",
    "        objective=\"Correlate typing pauses with downstream prompt quality shifts\",\n",
    "        owner=\"insights\",\n",
    "        tags=[\"telemetry\", \"latency\"],\n",
    "        confidence=0.55,\n",
    "        votes=3,\n",
    "        stage=\"analysis\",\n",
    "        data_points=1480,\n",
    "    ),\n",
    "    Hypothesis(\n",
    "        key=\"hyp-token-priming\",\n",
    "        title=\"Token priming uplift\",\n",
    "        objective=\"Front-load clear intent to cut clarification turns by 20%\",\n",
    "        owner=\"shared\",\n",
    "        tags=[\"prompting\", \"efficiency\"],\n",
    "        confidence=0.61,\n",
    "        votes=5,\n",
    "        stage=\"design\",\n",
    "        data_points=990,\n",
    "    ),\n",
    "    Hypothesis(\n",
    "        key=\"hyp-ops-memory\",\n",
    "        title=\"Ops deck memory\",\n",
    "        objective=\"Blend ops log context into TailLog feed for faster incident triage\",\n",
    "        owner=\"ops\",\n",
    "        tags=[\"ops\", \"observability\"],\n",
    "        confidence=0.48,\n",
    "        votes=2,\n",
    "        stage=\"ideation\",\n",
    "        data_points=420,\n",
    "    ),\n",
    "]\n",
    "\n",
    "sample_tests = [\n",
    "    TestCase(\n",
    "        key=\"test-pause-density\",\n",
    "        hypothesis_key=\"hyp-pause-density\",\n",
    "        name=\"Pause cluster detection\",\n",
    "        description=\"Detect meaningful pause clusters within 2s windows\",\n",
    "        metric=\"precision\",\n",
    "        target=0.78,\n",
    "        stage=\"lab\",\n",
    "        owner=\"instrumentation\",\n",
    "    ),\n",
    "    TestCase(\n",
    "        key=\"test-pause-quality\",\n",
    "        hypothesis_key=\"hyp-pause-density\",\n",
    "        name=\"Quality delta tracking\",\n",
    "        description=\"Relate pause clusters to quality score deltas\",\n",
    "        metric=\"pearson_r\",\n",
    "        target=0.52,\n",
    "        stage=\"analysis\",\n",
    "    ),\n",
    "    TestCase(\n",
    "        key=\"test-token-priming\",\n",
    "        hypothesis_key=\"hyp-token-priming\",\n",
    "        name=\"Prompt priming drop\",\n",
    "        description=\"Measure clarification drop after priming\",\n",
    "        metric=\"clarity_delta\",\n",
    "        target=0.2,\n",
    "        stage=\"pilot\",\n",
    "    ),\n",
    "    TestCase(\n",
    "        key=\"test-ops-tail\",\n",
    "        hypothesis_key=\"hyp-ops-memory\",\n",
    "        name=\"Ops log surfacing\",\n",
    "        description=\"Surface ops events inside TailLog\",\n",
    "        metric=\"triage_time\",\n",
    "        target=12,\n",
    "        stage=\"ideation\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "registry = HypothesisRegistry(sample_hypotheses, sample_tests)\n",
    "rng = np.random.default_rng(42)\n",
    "for test in sample_tests:\n",
    "    for _ in range(rng.integers(2, 6)):\n",
    "        jitter = rng.uniform(0.9, 1.15)\n",
    "        value = float(np.round(test.target * jitter, 3))\n",
    "        registry.log_run(\n",
    "            test_key=test.key,\n",
    "            value=value,\n",
    "            notes=\"seeded import\",\n",
    "            sample_size=int(rng.integers(80, 220)),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "903375c3",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nihil\\AppData\\Local\\Temp\\ipykernel_32780\\4193028259.py:9: DeprecationWarning:\n",
      "\n",
      "datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e549a27f8a4342887e8258bef57810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<div class='lab-panel'><h3>Meta grid</h3></div>\"), HBox(children=(HTML(value='\\n   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the interactive control surface.\n",
    "OPS_API_BASE = os.environ.get(\"CHATAI_LAB_API\", os.environ.get(\"CHATAI_API_BASE_URL\", \"http://localhost:8000\"))\n",
    "\n",
    "def fetch_tail_log(limit: int = 12):\n",
    "    fallback = [\n",
    "        {\n",
    "            \"message\": \"Tail log endpoint unavailable. Using notebook change-log instead.\",\n",
    "            \"source\": \"notebook\",\n",
    "            \"createdAt\": datetime.utcnow().isoformat(),\n",
    "        }\n",
    "    ]\n",
    "    if requests is None:\n",
    "        return fallback\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{OPS_API_BASE.rstrip('/')}/api/tail-log\",\n",
    "            params={\"limit\": limit},\n",
    "            timeout=3.5,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except Exception as exc:\n",
    "        fallback[0][\"message\"] = f\"Tail log unavailable ({exc})\"\n",
    "        return fallback\n",
    "\n",
    "def make_stat_card(label: str, value: str, hint: str, accent: str) -> widgets.HTML:\n",
    "    return widgets.HTML(\n",
    "        value=f'''\n",
    "        <div class=\"stat-card\">\n",
    "            <h4 style=\"color:{accent}\">{label}</h4>\n",
    "            <p>{value}</p>\n",
    "            <small>{hint}</small>\n",
    "        </div>\n",
    "        '''\n",
    "    )\n",
    "\n",
    "def format_dataframe(df: pd.DataFrame, empty_message: str) -> widgets.HTML:\n",
    "    if df.empty:\n",
    "        return widgets.HTML(value=f\"<div class='lab-panel'><em>{empty_message}</em></div>\")\n",
    "    return widgets.HTML(value=df.to_html(index=False, classes=\"lab-table\"))\n",
    "\n",
    "stats_box = widgets.HBox(layout=widgets.Layout(justify_content=\"space-between\"))\n",
    "hypothesis_table_out = widgets.Output()\n",
    "test_table_out = widgets.Output()\n",
    "combine_output = widgets.Output()\n",
    "ops_log_out = widgets.Output(layout=widgets.Layout(max_height=\"240px\", overflow_y=\"auto\"))\n",
    "change_log_out = widgets.Output(layout=widgets.Layout(max_height=\"200px\", overflow_y=\"auto\"))\n",
    "plot_output = widgets.Output()\n",
    "vote_status = widgets.HTML()\n",
    "flow_panel = widgets.HTML(\n",
    "    value='''\n",
    "    <div class=\"lab-panel\">\n",
    "        <h3>Workflow explainer</h3>\n",
    "        <p>This workspace keeps hypotheses, tests, telemetry, and decisions together. Start at the top meta grid, design or import tests, run or simulate them, then use the voting matrix to choose which hypothesis graduates into the dedicated TestLab environment.</p>\n",
    "        <ul>\n",
    "            <li><strong>Inputs</strong>: hypotheses, experiment definitions, live ops/tail logs, telemetry tables.</li>\n",
    "            <li><strong>Actions</strong>: create/edit tests, combine runs, run simulations, cast votes, refresh ops context.</li>\n",
    "            <li><strong>Outputs</strong>: decision matrix, aggregated pass rates, live data volume, change-log trail.</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    '''\n",
    ")\n",
    "\n",
    "hyp_title_input = widgets.Text(description=\"Title\", placeholder=\"Latency clusters → quality\")\n",
    "hyp_objective_input = widgets.Textarea(description=\"Objective\", rows=3)\n",
    "hyp_tags_input = widgets.Text(description=\"Tags\", placeholder=\"comma,separated\")\n",
    "add_hyp_button = widgets.Button(description=\"Add hypothesis\", button_style=\"success\", icon=\"plus\")\n",
    "\n",
    "test_parent_dropdown = widgets.Dropdown(options=[], description=\"Hypothesis\")\n",
    "test_name_input = widgets.Text(description=\"Test name\")\n",
    "test_metric_input = widgets.Text(description=\"Metric\", placeholder=\"precision\")\n",
    "test_target_input = widgets.FloatText(description=\"Target\")\n",
    "test_desc_input = widgets.Textarea(description=\"Description\", rows=2)\n",
    "test_stage_dropdown = widgets.Dropdown(options=[(\"Ideation\", \"ideation\"), (\"Lab\", \"lab\"), (\"Pilot\", \"pilot\"), (\"Analysis\", \"analysis\")], description=\"Stage\")\n",
    "add_test_button = widgets.Button(description=\"Add test\", button_style=\"info\", icon=\"flask\")\n",
    "\n",
    "run_test_dropdown = widgets.Dropdown(options=[], description=\"Test\")\n",
    "run_intensity_slider = widgets.FloatSlider(description=\"Intensity\", min=0.8, max=1.2, step=0.05, value=1.0)\n",
    "run_jitter_slider = widgets.FloatSlider(description=\"Jitter\", min=0.01, max=0.25, step=0.01, value=0.08)\n",
    "run_button = widgets.Button(description=\"Run simulation\", button_style=\"warning\", icon=\"play\")\n",
    "\n",
    "vote_dropdown = widgets.Dropdown(options=[], description=\"Hypothesis\")\n",
    "vote_slider = widgets.IntSlider(description=\"Votes\", min=1, max=5, value=1)\n",
    "vote_button = widgets.Button(description=\"Cast votes\", button_style=\"primary\", icon=\"check\")\n",
    "\n",
    "combine_select = widgets.SelectMultiple(options=[], description=\"Tests\", layout=widgets.Layout(width=\"320px\", height=\"200px\"))\n",
    "combine_button = widgets.Button(description=\"Combine & analyze\", icon=\"link\", button_style=\"info\")\n",
    "\n",
    "viz_chart_selector = widgets.ToggleButtons(\n",
    "    options=[\n",
    "        (\"Pass rate vs votes\", \"pass-votes\"),\n",
    "        (\"Latency trend\", \"latency\"),\n",
    "        (\"Data volume\", \"volume\"),\n",
    "    ],\n",
    "    description=\"Graph\",\n",
    ")\n",
    "viz_min_runs = widgets.IntSlider(description=\"Min runs\", min=1, max=8, value=2)\n",
    "\n",
    "ops_refresh_button = widgets.Button(description=\"Refresh tail log\", icon=\"refresh\", button_style=\"info\")\n",
    "\n",
    "def refresh_options():\n",
    "    hyp_options = [(hyp.title, hyp.key) for hyp in registry.hypotheses.values()]\n",
    "    test_options = [(test.name, test.key) for test in registry.tests.values()]\n",
    "    if not hyp_options:\n",
    "        hyp_options = [(\"—\", \"\")]\n",
    "    test_parent_dropdown.options = hyp_options\n",
    "    vote_dropdown.options = hyp_options\n",
    "    run_test_dropdown.options = test_options or [(\"—\", \"\")]\n",
    "    combine_select.options = test_options or []\n",
    "\n",
    "def refresh_stats():\n",
    "    stats = registry.metrics()\n",
    "    cards = [\n",
    "        make_stat_card(\"Hypotheses\", f\"{stats['hypotheses']}\", \"tracked\", \"#9d7bff\"),\n",
    "        make_stat_card(\"Tests\", f\"{stats['tests']}\", \"in catalog\", \"#7af5a5\"),\n",
    "        make_stat_card(\"Runs\", f\"{stats['runs']}\", \"executed\", \"#ffb347\"),\n",
    "        make_stat_card(\"Votes\", f\"{stats['votes']}\", \"total\", \"#ff9ca8\"),\n",
    "        make_stat_card(\"Confidence\", f\"{stats['avg_confidence']:.2f}\", \"avg\", \"#7dd3fc\"),\n",
    "        make_stat_card(\"Data pts\", f\"{stats['data_points']}\", \"captured\", \"#f472b6\"),\n",
    "        make_stat_card(\"Changes\", f\"{stats['changes']}\", \"this session\", \"#c4b5fd\"),\n",
    "    ]\n",
    "    stats_box.children = cards\n",
    "\n",
    "def refresh_tables():\n",
    "    with hypothesis_table_out:\n",
    "        clear_output(wait=True)\n",
    "        display(format_dataframe(registry.hypotheses_frame(), \"No hypotheses yet.\"))\n",
    "    with test_table_out:\n",
    "        clear_output(wait=True)\n",
    "        display(format_dataframe(registry.tests_frame(), \"Add a test to populate this table.\"))\n",
    "    with change_log_out:\n",
    "        clear_output(wait=True)\n",
    "        for ts, message in reversed(registry.changelog[-6:]):\n",
    "            display(widgets.HTML(value=f\"<div class='ops-log-entry'><strong>{ts.strftime('%H:%M:%S')}</strong> · {message}</div>\"))\n",
    "\n",
    "def refresh_plot(*_):\n",
    "    df = registry.tests_frame()\n",
    "    if df.empty:\n",
    "        with plot_output:\n",
    "            clear_output(wait=True)\n",
    "            display(widgets.HTML(value=\"<em>No test data yet.</em>\"))\n",
    "            return\n",
    "    summary = registry.hypotheses_frame()\n",
    "    if viz_chart_selector.value == \"pass-votes\":\n",
    "        fig = px.scatter(\n",
    "            summary,\n",
    "            x=\"votes\",\n",
    "            y=\"avg_pass_rate\",\n",
    "            size=\"data_points\",\n",
    "            color=\"confidence\",\n",
    "            hover_name=\"title\",\n",
    "            title=\"Votes vs pass rate (bubble size = data volume)\",\n",
    "            range_y=[0, 1],\n",
    "            template=\"plotly_dark\",\n",
    "        )\n",
    "    elif viz_chart_selector.value == \"latency\":\n",
    "        results = registry.results_frame()\n",
    "        filtered = results.groupby(\"test_key\").filter(lambda grp: len(grp) >= viz_min_runs.value)\n",
    "        if filtered.empty:\n",
    "            fig = px.scatter(title=\"Not enough runs for latency trend\")\n",
    "        else:\n",
    "            fig = px.line(\n",
    "                filtered,\n",
    "                x=\"created_at\",\n",
    "                y=\"value\",\n",
    "                color=\"test_key\",\n",
    "                title=\"Metric trend over time\",\n",
    "                template=\"plotly_dark\",\n",
    "            )\n",
    "    else:\n",
    "        summary = registry.tests_frame()\n",
    "        fig = px.bar(\n",
    "            summary,\n",
    "            x=\"test\",\n",
    "            y=\"data_points\",\n",
    "            color=\"stage\",\n",
    "            title=\"Data volume per test\",\n",
    "            template=\"plotly_dark\",\n",
    "        )\n",
    "    with plot_output:\n",
    "        clear_output(wait=True)\n",
    "        fig.show()\n",
    "\n",
    "def refresh_ops_log(*_):\n",
    "    entries = fetch_tail_log(limit=15)\n",
    "    with ops_log_out:\n",
    "        clear_output(wait=True)\n",
    "        for entry in entries:\n",
    "            created = entry.get(\"createdAt\") or entry.get(\"created_at\")\n",
    "            try:\n",
    "                timestamp = datetime.fromisoformat(str(created).replace(\"Z\", \"\"))\n",
    "                stamp = timestamp.strftime(\"%H:%M:%S\")\n",
    "            except Exception:\n",
    "                stamp = \"—\"\n",
    "            display(\n",
    "                widgets.HTML(\n",
    "                    value=f\"<div class='ops-log-entry'><strong>{stamp}</strong> [{entry.get('source','ops')}] {entry.get('message')}</div>\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "def handle_add_hypothesis(_):\n",
    "    if not hyp_title_input.value or not hyp_objective_input.value:\n",
    "        return\n",
    "    tags = [tag.strip() for tag in hyp_tags_input.value.split(\",\") if tag.strip()]\n",
    "    registry.add_hypothesis(hyp_title_input.value, hyp_objective_input.value, tags=tags)\n",
    "    hyp_title_input.value = \"\"\n",
    "    hyp_objective_input.value = \"\"\n",
    "    hyp_tags_input.value = \"\"\n",
    "    refresh_options()\n",
    "    refresh_stats()\n",
    "    refresh_tables()\n",
    "\n",
    "def handle_add_test(_):\n",
    "    if not test_parent_dropdown.value or not test_name_input.value:\n",
    "        return\n",
    "    registry.add_test(\n",
    "        hypothesis_key=test_parent_dropdown.value,\n",
    "        name=test_name_input.value,\n",
    "        description=test_desc_input.value,\n",
    "        metric=test_metric_input.value or \"metric\",\n",
    "        target=test_target_input.value or 0.0,\n",
    "        stage=test_stage_dropdown.value,\n",
    "    )\n",
    "    test_name_input.value = \"\"\n",
    "    test_metric_input.value = \"\"\n",
    "    test_target_input.value = 0.0\n",
    "    test_desc_input.value = \"\"\n",
    "    refresh_options()\n",
    "    refresh_tables()\n",
    "\n",
    "def handle_run_test(_):\n",
    "    if not run_test_dropdown.value:\n",
    "        return\n",
    "    registry.simulate_run(\n",
    "        run_test_dropdown.value,\n",
    "        intensity=run_intensity_slider.value,\n",
    "        jitter=run_jitter_slider.value,\n",
    "    )\n",
    "    refresh_stats()\n",
    "    refresh_tables()\n",
    "    refresh_plot()\n",
    "\n",
    "def handle_vote(_):\n",
    "    if not vote_dropdown.value:\n",
    "        return\n",
    "    registry.cast_vote(vote_dropdown.value, vote_slider.value)\n",
    "    vote_status.value = f\"<div class='vote-pill'>Votes applied · +{vote_slider.value}</div>\"\n",
    "    refresh_stats()\n",
    "    refresh_tables()\n",
    "    refresh_plot()\n",
    "\n",
    "def handle_combine(_):\n",
    "    selected = list(combine_select.value)\n",
    "    combo = registry.combine_tests(selected)\n",
    "    with combine_output:\n",
    "        clear_output(wait=True)\n",
    "        if combo.empty:\n",
    "            display(widgets.HTML(value=\"<em>Select at least one test.</em>\"))\n",
    "        else:\n",
    "            display(format_dataframe(combo, \"\"))\n",
    "\n",
    "add_hyp_button.on_click(handle_add_hypothesis)\n",
    "add_test_button.on_click(handle_add_test)\n",
    "run_button.on_click(handle_run_test)\n",
    "vote_button.on_click(handle_vote)\n",
    "combine_button.on_click(handle_combine)\n",
    "viz_chart_selector.observe(refresh_plot, names=\"value\")\n",
    "viz_min_runs.observe(refresh_plot, names=\"value\")\n",
    "ops_refresh_button.on_click(refresh_ops_log)\n",
    "\n",
    "refresh_options()\n",
    "refresh_stats()\n",
    "refresh_tables()\n",
    "refresh_plot()\n",
    "refresh_ops_log()\n",
    "\n",
    "hypothesis_panel = widgets.VBox(\n",
    "    [\n",
    "        widgets.HTML(value=\"<h3>Hypothesis registry</h3>\"),\n",
    "        hypothesis_table_out,\n",
    "        widgets.HBox([hyp_title_input, hyp_tags_input]),\n",
    "        hyp_objective_input,\n",
    "        add_hyp_button,\n",
    "    ],\n",
    "    layout=widgets.Layout(width=\"48%\"),\n",
    ")\n",
    "\n",
    "test_panel = widgets.VBox(\n",
    "    [\n",
    "        widgets.HTML(value=\"<h3>Experiment designer</h3>\"),\n",
    "        test_table_out,\n",
    "        test_parent_dropdown,\n",
    "        test_name_input,\n",
    "        widgets.HBox([test_metric_input, test_target_input]),\n",
    "        test_desc_input,\n",
    "        test_stage_dropdown,\n",
    "        add_test_button,\n",
    "    ],\n",
    "    layout=widgets.Layout(width=\"48%\"),\n",
    ")\n",
    "\n",
    "run_panel = widgets.VBox(\n",
    "    [\n",
    "        widgets.HTML(value=\"<h3>Run & monitor tests</h3>\"),\n",
    "        widgets.HBox([run_test_dropdown, run_intensity_slider, run_jitter_slider]),\n",
    "        run_button,\n",
    "        widgets.HTML(value=\"<h4>Combined / cross-reference</h4>\"),\n",
    "        combine_select,\n",
    "        combine_button,\n",
    "        combine_output,\n",
    "    ]\n",
    ")\n",
    "\n",
    "voting_panel = widgets.VBox(\n",
    "    [\n",
    "        widgets.HTML(value=\"<h3>Voting + decision matrix</h3>\"),\n",
    "        vote_dropdown,\n",
    "        vote_slider,\n",
    "        vote_button,\n",
    "        vote_status,\n",
    "        widgets.HTML(value=\"<h4>Recent changes</h4>\"),\n",
    "        change_log_out,\n",
    "    ]\n",
    ")\n",
    "\n",
    "viz_panel = widgets.VBox(\n",
    "    [\n",
    "        widgets.HTML(value=\"<h3>Interactive data wall</h3>\"),\n",
    "        widgets.HBox([viz_chart_selector, viz_min_runs]),\n",
    "        plot_output,\n",
    "    ]\n",
    ")\n",
    "\n",
    "ops_panel = widgets.VBox(\n",
    "    [\n",
    "        widgets.HTML(value=\"<h3>Ops / Tail console</h3>\"),\n",
    "        ops_refresh_button,\n",
    "        ops_log_out,\n",
    "    ]\n",
    ")\n",
    "\n",
    "dashboard = widgets.VBox(\n",
    "    [\n",
    "        widgets.HTML(value=\"<div class='lab-panel'><h3>Meta grid</h3></div>\"),\n",
    "        stats_box,\n",
    "        flow_panel,\n",
    "        widgets.HBox([hypothesis_panel, test_panel], layout=widgets.Layout(justify_content=\"space-between\")),\n",
    "        widgets.HBox([run_panel, voting_panel], layout=widgets.Layout(justify_content=\"space-between\")),\n",
    "        viz_panel,\n",
    "        ops_panel,\n",
    "    ],\n",
    "    layout=widgets.Layout(gap=\"1.5rem\"),\n",
    ")\n",
    "\n",
    "display(dashboard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a138f9f0-1ad5-41b6-95f1-2adbc3fea7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Jim!\n"
     ]
    }
   ],
   "source": [
    "name = \"Jim\"\n",
    "print(\"Hi \" + name + \"!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556b0b3-23ac-44da-85e0-ebefa236a210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
